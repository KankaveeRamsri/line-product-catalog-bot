import csv
import random
import requests
from bs4 import BeautifulSoup
import json
from pathlib import Path

# ---------------- CONFIG ----------------
csv_filenames = ['bnn_links\\notebook.csv',
                 'bnn_links\smartphone-and-accessories.csv',
                 'bnn_links\gaming-gear.csv'
                 ]  # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô \\ ‡πÄ‡∏õ‡πá‡∏ô / ‡πÄ‡∏û‡∏∑‡πà‡∏≠ cross-platform
output_dir = Path("bnn_details_json")
output_dir.mkdir(exist_ok=True)
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/114.0.0.0 Safari/537.36"
}
# ----------------------------------------


def scrape_product_detail(url: str) -> dict:
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å product page (‡πÄ‡∏â‡∏û‡∏≤‡∏∞ div.product-detail-summary)"""
    try:
        resp = requests.get(url, headers=headers, timeout=20)
        soup = BeautifulSoup(resp.text, "html.parser")
        summary = soup.select_one("div.product-detail-summary")
        if not summary:
            return {"error": "product-detail-summary not found"}

        # ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠
        title = summary.select_one("h1.product-name")
        brand = summary.select_one(".brand-value")
        sku = summary.select_one(".sku-number-value")
        desc = summary.select_one(".product-short-description p")

        # ‡∏™‡πÄ‡∏õ‡∏Ñ‡∏à‡∏≤‡∏Å <li>
        specs = {}
        for li in summary.select(".product-short-description li"):
            text = li.get_text(" ", strip=True)
            if ":" in text:
                key, value = text.split(":", 1)
                specs[key.strip()] = value.strip()

        # ‡∏õ‡πâ‡∏≤‡∏¢ label ‡∏à‡∏≤‡∏Å alt
        labels = []
        for img in summary.select(".product-label-container img"):
            alt = img.get("alt")
            if alt:
                labels.append(alt.strip())
        
        # ‡∏î‡∏∂‡∏á‡∏£‡∏π‡∏õ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å gallery (swiper-container.gallery-thumbs)
        image_urls = []
        for img_tag in soup.select('.gallery-thumbs img'):
            src = img_tag.get("src")
            if src and src.startswith("http"):
                image_urls.append(src.strip())
        
        # ================= ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô =================

        # ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (selling price)
        selling_price_tag = soup.select_one(".selling-price")
        selling_price = selling_price_tag.get_text(strip=True) if selling_price_tag else ""

        # ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏•‡∏î (srp price)
        srp_price_tag = soup.select_one(".srp-price")
        srp_price = srp_price_tag.get_text(strip=True) if srp_price_tag else ""

        # ‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô
        warranty_tag = soup.select_one(".product-warranty .caption")
        warranty = warranty_tag.get_text(strip=True) if warranty_tag else ""

        return {
            "title": title.get_text(strip=True) if title else "",
            "brand": brand.get_text(strip=True) if brand else "",
            "sku": sku.get_text(strip=True) if sku else "",
            "description": desc.get_text(" ", strip=True) if desc else "",
            "specs": specs,
            "labels": labels,
            "imageUrl": image_urls[0] if image_urls else "",
            "gallery": image_urls,  # ‡∏£‡∏π‡∏õ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö list
            "selling_price": selling_price,
            "srp_price": srp_price,
            "warranty": warranty

        }
    except Exception as e:
        return {"error": str(e)}


# ---------------- START ----------------

for csv_filename in csv_filenames:
    category_name = Path(csv_filename).stem  # ‡πÄ‡∏ä‡πà‡∏ô notebook
    output_file = output_dir / f"{category_name}_details.json"

    print(f"\nüìÇ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å: {csv_filename}")
    all_results = []

    # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV
    with open(csv_filename, newline='', encoding='utf-8-sig') as csvfile:
        reader = csv.DictReader(csvfile)
        rows = list(reader)

        # ‡∏™‡∏∏‡πà‡∏° 10 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ (‡∏ñ‡πâ‡∏≤‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ 10 ‡∏Å‡πá‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)
        sample_size = min(10, len(rows))
        sampled_rows = random.sample(rows, sample_size)

        for i, row in enumerate(sampled_rows, 1):
            name = row.get('name', '').strip()
            url = row.get('url', '').strip()

            # print(f'{i}. üì¶ ‡∏£‡∏∏‡πà‡∏ô: {name}')
            # print(f'   üîó ‡∏•‡∏¥‡∏á‡∏Å‡πå: {url}')

            detail = scrape_product_detail(url)
            detail["source_name"] = name
            detail["url"] = url

            all_results.append(detail)
            print(f'   ‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß ({len(detail.keys())} ‡∏ü‡∏¥‡∏•‡∏î‡πå)')
            print('-' * 60)

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å JSON
    with output_file.open("w", encoding="utf-8") as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢: {output_file}")
